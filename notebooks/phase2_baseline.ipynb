{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Uploading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:20:00.157234Z",
     "iopub.status.busy": "2025-05-27T14:20:00.156832Z",
     "iopub.status.idle": "2025-05-27T14:22:50.160170Z",
     "shell.execute_reply": "2025-05-27T14:22:50.159541Z",
     "shell.execute_reply.started": "2025-05-27T14:20:00.157205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ df shape: (71854, 4)\n",
      "aspect\n",
      "clarity_negative       15697\n",
      "soundness_negative     15662\n",
      "clarity_positive       13095\n",
      "soundness_positive     12780\n",
      "motivation_positive    11121\n",
      "Name: count, dtype: int64\n",
      "                                          paper_text  \\\n",
      "1  Deep neural networks (DNNs) have achieved impe...   \n",
      "2  Deep neural networks (DNNs) have achieved impe...   \n",
      "3  Deep neural networks (DNNs) have achieved impe...   \n",
      "6  Deep neural networks (DNNs) have achieved impe...   \n",
      "8  A long-term goal in artificial intelligence is...   \n",
      "\n",
      "                                         review_text               aspect  \\\n",
      "1  This work studies the predictive uncertainty i...  motivation_positive   \n",
      "2  This work studies the predictive uncertainty i...     clarity_positive   \n",
      "3  This work studies the predictive uncertainty i...   soundness_negative   \n",
      "6  This work studies the predictive uncertainty i...   soundness_negative   \n",
      "8  Summary : This paper proposes a new approach t...  motivation_positive   \n",
      "\n",
      "                                     aspect_sentence  \n",
      "1  The issue researched in this work is of signif...  \n",
      "2  The motivation , research issues and the propo...  \n",
      "3  The current recommendation is Weak Reject beca...  \n",
      "6  The novelty and significance of fine-tuning th...  \n",
      "8                       This paper is well motivated  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────\n",
    "DATASET_ROOT = \"/kaggle/input/is584dataset/dataset\"  # adjust if needed\n",
    "ASPECT_PATH  = os.path.join(DATASET_ROOT, \"aspect_data\", \"review_with_aspect.jsonl\")\n",
    "CONFS        = [\n",
    "    \"ICLR_2017\",\"ICLR_2018\",\"ICLR_2019\",\"ICLR_2020\",\n",
    "    \"NIPS_2016\",\"NIPS_2017\",\"NIPS_2018\",\"NIPS_2019\"\n",
    "]\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def extract_text_fields(doc):\n",
    "    \"\"\"Recursively collect all string values >50 chars from a nested JSON.\"\"\"\n",
    "    texts = []\n",
    "    def recurse(o):\n",
    "        if isinstance(o, str):\n",
    "            if len(o.strip()) > 50:\n",
    "                texts.append(o.strip())\n",
    "        elif isinstance(o, list):\n",
    "            for item in o:\n",
    "                recurse(item)\n",
    "        elif isinstance(o, dict):\n",
    "            for v in o.values():\n",
    "                recurse(v)\n",
    "    recurse(doc)\n",
    "    return \"\\n\\n\".join(texts) if texts else None\n",
    "\n",
    "# 1) Load full paper texts\n",
    "paper_records = []\n",
    "for conf in CONFS:\n",
    "    content_dir = os.path.join(DATASET_ROOT, conf, f\"{conf}_content\")\n",
    "    if not os.path.isdir(content_dir):\n",
    "        continue\n",
    "    for fn in os.listdir(content_dir):\n",
    "        if not fn.endswith(\"_content.json\"):\n",
    "            continue\n",
    "        path = os.path.join(content_dir, fn)\n",
    "        doc = json.load(open(path, \"r\"))\n",
    "        sid = fn.replace(\"_content.json\", \"\")\n",
    "        text = extract_text_fields(doc)\n",
    "        paper_records.append({\"submission_id\": sid, \"paper_text\": text})\n",
    "paper_df = pd.DataFrame(paper_records)\n",
    "\n",
    "# 2) Flatten aspect spans\n",
    "asp_recs = []\n",
    "with open(ASPECT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj    = json.loads(line)\n",
    "        sid    = obj[\"id\"]\n",
    "        revtxt = obj[\"text\"]\n",
    "        for st, ed, lbl in obj[\"labels\"]:\n",
    "            span = revtxt[st:ed].strip()\n",
    "            if not span:\n",
    "                continue\n",
    "            asp_recs.append({\n",
    "                \"submission_id\":   sid,\n",
    "                \"review_text\":     revtxt,\n",
    "                \"aspect\":          lbl,\n",
    "                \"aspect_sentence\": span\n",
    "            })\n",
    "aspect_df = pd.DataFrame(asp_recs)\n",
    "\n",
    "# 3) Merge papers + aspects, drop missing texts\n",
    "df = aspect_df.merge(paper_df, on=\"submission_id\", how=\"left\")\n",
    "df = df[df[\"paper_text\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# 4) Filter to core aspects\n",
    "core = [\"clarity_positive\",\"clarity_negative\",\"soundness_positive\",\"soundness_negative\",\"motivation_positive\",\"motivation_negative\"]\n",
    "df = df[df[\"aspect\"].isin(core)]\n",
    "\n",
    "# 5) Final fields\n",
    "df = df[[\"paper_text\",\"review_text\",\"aspect\",\"aspect_sentence\"]]\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"✔ df shape:\", df.shape)\n",
    "print(df[\"aspect\"].value_counts().head())\n",
    "print(df.head())\n",
    "\n",
    "# ——— Sanitize text columns to remove/replace problematic chars ———\n",
    "for col in [\"paper_text\", \"review_text\", \"aspect_sentence\"]:\n",
    "    df[col] = df[col].apply(\n",
    "        lambda x: x.encode(\"utf-8\", \"ignore\").decode(\"utf-8\") if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "# Now save without error\n",
    "df.to_csv(\"phase2_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:22:50.161629Z",
     "iopub.status.busy": "2025-05-27T14:22:50.161322Z",
     "iopub.status.idle": "2025-05-27T14:24:00.314375Z",
     "shell.execute_reply": "2025-05-27T14:24:00.313661Z",
     "shell.execute_reply.started": "2025-05-27T14:22:50.161609Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Loaded 8850 papers\n",
      "→ Loaded 148086 aspect‐span entries\n",
      "→ Loaded 28122 raw review entries\n",
      "→ Selected 8780 best‐confidence reviews\n",
      "✔ Final dataset: 8704 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ─── CONFIG ───────────────────────────────────────────────────────────\n",
    "DATASET_ROOT = \"/kaggle/input/is584dataset/dataset\"  # adjust if needed\n",
    "ASPECT_PATH  = os.path.join(DATASET_ROOT, \"aspect_data\", \"review_with_aspect.jsonl\")\n",
    "CONFS        = [\n",
    "    \"ICLR_2017\",\"ICLR_2018\",\"ICLR_2019\",\"ICLR_2020\",\n",
    "    \"NIPS_2016\",\"NIPS_2017\",\"NIPS_2018\",\"NIPS_2019\"\n",
    "]\n",
    "CORE_ASPECTS = {\n",
    "    \"clarity_positive\",\"clarity_negative\",\n",
    "    \"soundness_positive\",\"soundness_negative\",\n",
    "    \"motivation_positive\",\"motivation_negative\"\n",
    "}\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def extract_text_fields(doc):\n",
    "    \"\"\"Collect all string values ≥50 chars from nested JSON.\"\"\"\n",
    "    texts = []\n",
    "    def recurse(o):\n",
    "        if isinstance(o, str):\n",
    "            if len(o.strip()) > 50:\n",
    "                texts.append(o.strip())\n",
    "        elif isinstance(o, dict):\n",
    "            for v in o.values():\n",
    "                recurse(v)\n",
    "        elif isinstance(o, list):\n",
    "            for item in o:\n",
    "                recurse(item)\n",
    "    recurse(doc)\n",
    "    return \"\\n\\n\".join(texts) if texts else None\n",
    "\n",
    "# 1) Load full paper texts\n",
    "paper_records = []\n",
    "for conf in CONFS:\n",
    "    content_dir = os.path.join(DATASET_ROOT, conf, f\"{conf}_content\")\n",
    "    if not os.path.isdir(content_dir):\n",
    "        continue\n",
    "    for fn in os.listdir(content_dir):\n",
    "        if not fn.endswith(\"_content.json\"):\n",
    "            continue\n",
    "        sid = fn.replace(\"_content.json\", \"\")\n",
    "        doc = json.load(open(os.path.join(content_dir, fn), \"r\"))\n",
    "        text = extract_text_fields(doc)\n",
    "        if text:\n",
    "            paper_records.append({\"submission_id\": sid, \"paper_text\": text})\n",
    "paper_df = pd.DataFrame(paper_records)\n",
    "print(f\"→ Loaded {len(paper_df)} papers\")\n",
    "\n",
    "# 2) Load aspect spans\n",
    "asp_recs = []\n",
    "with open(ASPECT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj    = json.loads(line)\n",
    "        sid    = obj[\"id\"]\n",
    "        revtxt = obj[\"text\"]\n",
    "        for st, ed, lbl in obj[\"labels\"]:\n",
    "            span = revtxt[st:ed].strip()\n",
    "            if span:\n",
    "                asp_recs.append({\n",
    "                    \"submission_id\":   sid,\n",
    "                    \"review_text\":     revtxt,\n",
    "                    \"aspect\":          lbl,\n",
    "                    \"aspect_sentence\": span\n",
    "                })\n",
    "aspect_df = pd.DataFrame(asp_recs)\n",
    "print(f\"→ Loaded {len(aspect_df)} aspect‐span entries\")\n",
    "\n",
    "# 3) Load all reviews, parse rating+confidence, pick top‐confidence one per paper\n",
    "def parse_lead_int(s):\n",
    "    \"\"\"If s like '4: ...', return 4 else 0.\"\"\"\n",
    "    if not isinstance(s, str) or \":\" not in s:\n",
    "        return 0\n",
    "    try:\n",
    "        return int(s.split(\":\",1)[0])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "rev_recs = []\n",
    "for conf in CONFS:\n",
    "    rev_dir = os.path.join(DATASET_ROOT, conf, f\"{conf}_review\")\n",
    "    if not os.path.isdir(rev_dir):\n",
    "        continue\n",
    "    for fn in os.listdir(rev_dir):\n",
    "        if not fn.endswith(\"_review.json\"):\n",
    "            continue\n",
    "        data = json.load(open(os.path.join(rev_dir, fn), \"r\"))\n",
    "        root = data.get(\"root\", data)\n",
    "        sid  = root.get(\"id\", fn.replace(\"_review.json\",\"\"))\n",
    "        for rv in root.get(\"reviews\", []):\n",
    "            txt = rv.get(\"review\",\"\").strip()\n",
    "            if not txt:\n",
    "                continue\n",
    "            rating     = parse_lead_int(rv.get(\"rating\",\"\"))\n",
    "            confidence = parse_lead_int(rv.get(\"confidence\",\"\"))\n",
    "            rev_recs.append({\n",
    "                \"submission_id\": sid,\n",
    "                \"review\":        txt,\n",
    "                \"rating\":        rating,\n",
    "                \"confidence\":    confidence\n",
    "            })\n",
    "\n",
    "rev_df = pd.DataFrame(rev_recs)\n",
    "print(f\"→ Loaded {len(rev_df)} raw review entries\")\n",
    "\n",
    "best_rev = (\n",
    "    rev_df\n",
    "    .sort_values(\n",
    "        [\"submission_id\",\"confidence\",\"rating\"],\n",
    "        ascending=[True, False, False]\n",
    "    )\n",
    "    .groupby(\"submission_id\", as_index=False)\n",
    "    .first()[[\"submission_id\",\"review\",\"confidence\"]]\n",
    ")\n",
    "print(f\"→ Selected {len(best_rev)} best‐confidence reviews\")\n",
    "\n",
    "# 4) Merge papers + aspects + chosen reviews\n",
    "df = (\n",
    "    aspect_df\n",
    "    .merge(paper_df,  on=\"submission_id\", how=\"left\")\n",
    "    .merge(best_rev,  on=\"submission_id\", how=\"left\")\n",
    "    .dropna(subset=[\"paper_text\",\"review_text\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 5) Keep only core aspects\n",
    "df = df[df[\"aspect\"].isin(CORE_ASPECTS)]\n",
    "\n",
    "# 6) Deduplicate per paper+aspect: pick the longest aspect_sentence\n",
    "df[\"span_len\"] = df[\"aspect_sentence\"].str.len()\n",
    "df = (\n",
    "    df\n",
    "    .sort_values(\n",
    "        [\"submission_id\", \"aspect\", \"span_len\"],\n",
    "        ascending=[True, True, False]\n",
    "    )\n",
    "    .drop_duplicates([\"submission_id\"], keep=\"first\")\n",
    "    .drop(columns=\"span_len\")\n",
    ")\n",
    "\n",
    "# 7) Sanitize and save\n",
    "for col in [\"paper_text\", \"review\", \"aspect_sentence\"]:\n",
    "    df[col] = (df[col]\n",
    "               .astype(str)\n",
    "               .apply(lambda x: x.encode(\"utf-8\",\"ignore\")\n",
    "                                .decode(\"utf-8\")))\n",
    "df.to_csv(\"phase2_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✔ Final dataset: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:24:00.315587Z",
     "iopub.status.busy": "2025-05-27T14:24:00.315287Z",
     "iopub.status.idle": "2025-05-27T14:24:00.336310Z",
     "shell.execute_reply": "2025-05-27T14:24:00.335612Z",
     "shell.execute_reply.started": "2025-05-27T14:24:00.315561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>aspect</th>\n",
       "      <th>aspect_sentence</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>review</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121061</th>\n",
       "      <td>ICLR_2017_1</td>\n",
       "      <td>This is a very interesting and fairly easy to ...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>As a non-expert on this topic , it was easy to...</td>\n",
       "      <td>MAKING NEURAL PROGRAMMING ARCHITECTURES GENERA...</td>\n",
       "      <td>This paper improves significantly upon the ori...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31763</th>\n",
       "      <td>ICLR_2017_10</td>\n",
       "      <td>* *Edit : Based on the discussion below , my m...</td>\n",
       "      <td>clarity_negative</td>\n",
       "      <td>There are also other typos throughout</td>\n",
       "      <td>Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH ...</td>\n",
       "      <td>**Edit: Based on the discussion below, my main...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135807</th>\n",
       "      <td>ICLR_2017_100</td>\n",
       "      <td>In this paper , the authors use a separate int...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>The organization is generally very clear</td>\n",
       "      <td>INTROSPECTION:ACCELERATING NEURAL NETWORK TRAI...</td>\n",
       "      <td>EDIT: Updated score. See additional comment.\\n...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122259</th>\n",
       "      <td>ICLR_2017_101</td>\n",
       "      <td>This was an interesting paper . The algorithm ...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>The algorithm seems clear , the problem well-r...</td>\n",
       "      <td>The task of hyperparameter optimization is bec...</td>\n",
       "      <td>This paper discusses Hyperband, an extension o...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>ICLR_2017_102</td>\n",
       "      <td>The paper proposes a new memory access scheme ...</td>\n",
       "      <td>clarity_negative</td>\n",
       "      <td>The difference to the properties of normal NTM...</td>\n",
       "      <td>Recent work on neural Turing machines (NTMs) (...</td>\n",
       "      <td>*** Paper Summary ***\\n\\nThis paper formalizes...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59132</th>\n",
       "      <td>ICLR_2017_103</td>\n",
       "      <td>This paper points out that you can take an LST...</td>\n",
       "      <td>clarity_negative</td>\n",
       "      <td>Unfortunately , this simple , effective and in...</td>\n",
       "      <td>Recurrent neural networks (RNNs), including ga...</td>\n",
       "      <td>This paper introduces a novel RNN architecture...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124269</th>\n",
       "      <td>ICLR_2017_104</td>\n",
       "      <td>The authors propose a recurrent neural network...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>In general the paper is well written and quite...</td>\n",
       "      <td>In order to plan and act effectively, agent-ba...</td>\n",
       "      <td>[UPDATE]\\nAfter going through the response fro...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66336</th>\n",
       "      <td>ICLR_2017_105</td>\n",
       "      <td>This paper explores ensemble optimisation in t...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>The paper is well written and accessible .</td>\n",
       "      <td>EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES...</td>\n",
       "      <td>This paper explores ensemble optimisation in t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66731</th>\n",
       "      <td>ICLR_2017_106</td>\n",
       "      <td>In this paper a well known soft mixture of exp...</td>\n",
       "      <td>motivation_positive</td>\n",
       "      <td>This is clearly an interesting direction of fu...</td>\n",
       "      <td>Transferring knowledge from prior source tasks...</td>\n",
       "      <td>In this paper a well known soft mixture of exp...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77127</th>\n",
       "      <td>ICLR_2017_107</td>\n",
       "      <td>This paper proposes an approach to learning wo...</td>\n",
       "      <td>clarity_positive</td>\n",
       "      <td>The paper is clearly written</td>\n",
       "      <td>MULTI-VIEW RECURRENT NEURAL ACOUSTIC WORD EMBE...</td>\n",
       "      <td>This paper proposes an approach to learning wo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        submission_id                                        review_text  \\\n",
       "121061    ICLR_2017_1  This is a very interesting and fairly easy to ...   \n",
       "31763    ICLR_2017_10  * *Edit : Based on the discussion below , my m...   \n",
       "135807  ICLR_2017_100  In this paper , the authors use a separate int...   \n",
       "122259  ICLR_2017_101  This was an interesting paper . The algorithm ...   \n",
       "419     ICLR_2017_102  The paper proposes a new memory access scheme ...   \n",
       "59132   ICLR_2017_103  This paper points out that you can take an LST...   \n",
       "124269  ICLR_2017_104  The authors propose a recurrent neural network...   \n",
       "66336   ICLR_2017_105  This paper explores ensemble optimisation in t...   \n",
       "66731   ICLR_2017_106  In this paper a well known soft mixture of exp...   \n",
       "77127   ICLR_2017_107  This paper proposes an approach to learning wo...   \n",
       "\n",
       "                     aspect  \\\n",
       "121061     clarity_positive   \n",
       "31763      clarity_negative   \n",
       "135807     clarity_positive   \n",
       "122259     clarity_positive   \n",
       "419        clarity_negative   \n",
       "59132      clarity_negative   \n",
       "124269     clarity_positive   \n",
       "66336      clarity_positive   \n",
       "66731   motivation_positive   \n",
       "77127      clarity_positive   \n",
       "\n",
       "                                          aspect_sentence  \\\n",
       "121061  As a non-expert on this topic , it was easy to...   \n",
       "31763               There are also other typos throughout   \n",
       "135807           The organization is generally very clear   \n",
       "122259  The algorithm seems clear , the problem well-r...   \n",
       "419     The difference to the properties of normal NTM...   \n",
       "59132   Unfortunately , this simple , effective and in...   \n",
       "124269  In general the paper is well written and quite...   \n",
       "66336          The paper is well written and accessible .   \n",
       "66731   This is clearly an interesting direction of fu...   \n",
       "77127                        The paper is clearly written   \n",
       "\n",
       "                                               paper_text  \\\n",
       "121061  MAKING NEURAL PROGRAMMING ARCHITECTURES GENERA...   \n",
       "31763   Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH ...   \n",
       "135807  INTROSPECTION:ACCELERATING NEURAL NETWORK TRAI...   \n",
       "122259  The task of hyperparameter optimization is bec...   \n",
       "419     Recent work on neural Turing machines (NTMs) (...   \n",
       "59132   Recurrent neural networks (RNNs), including ga...   \n",
       "124269  In order to plan and act effectively, agent-ba...   \n",
       "66336   EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES...   \n",
       "66731   Transferring knowledge from prior source tasks...   \n",
       "77127   MULTI-VIEW RECURRENT NEURAL ACOUSTIC WORD EMBE...   \n",
       "\n",
       "                                                   review  confidence  \n",
       "121061  This paper improves significantly upon the ori...           5  \n",
       "31763   **Edit: Based on the discussion below, my main...           5  \n",
       "135807  EDIT: Updated score. See additional comment.\\n...           5  \n",
       "122259  This paper discusses Hyperband, an extension o...           5  \n",
       "419     *** Paper Summary ***\\n\\nThis paper formalizes...           4  \n",
       "59132   This paper introduces a novel RNN architecture...           4  \n",
       "124269  [UPDATE]\\nAfter going through the response fro...           5  \n",
       "66336   This paper explores ensemble optimisation in t...           4  \n",
       "66731   In this paper a well known soft mixture of exp...           4  \n",
       "77127   This paper proposes an approach to learning wo...           4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:24:00.338251Z",
     "iopub.status.busy": "2025-05-27T14:24:00.338042Z",
     "iopub.status.idle": "2025-05-27T14:24:00.346817Z",
     "shell.execute_reply": "2025-05-27T14:24:00.346153Z",
     "shell.execute_reply.started": "2025-05-27T14:24:00.338235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect\n",
       "clarity_negative       6245\n",
       "clarity_positive       1959\n",
       "motivation_positive     219\n",
       "motivation_negative     137\n",
       "soundness_negative      111\n",
       "soundness_positive       33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"aspect\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:24:00.347742Z",
     "iopub.status.busy": "2025-05-27T14:24:00.347489Z",
     "iopub.status.idle": "2025-05-27T14:24:18.601167Z",
     "shell.execute_reply": "2025-05-27T14:24:18.600392Z",
     "shell.execute_reply.started": "2025-05-27T14:24:00.347718Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall aspect distribution:\n",
      "aspect\n",
      "clarity_negative       6245\n",
      "clarity_positive       1959\n",
      "motivation_positive     219\n",
      "motivation_negative     137\n",
      "soundness_negative      111\n",
      "soundness_positive       33\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Train+Val shape: (6963, 7)\n",
      "Test shape:        (1741, 7)\n",
      "\n",
      "Train shape: (6092, 7)\n",
      "Val shape:   (871, 7)\n",
      "Test shape:  (1741, 7)\n",
      "\n",
      "Class distribution in TRAIN set:\n",
      "aspect\n",
      "clarity_negative       4371\n",
      "clarity_positive       1371\n",
      "motivation_positive     153\n",
      "motivation_negative      96\n",
      "soundness_negative       78\n",
      "soundness_positive       23\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Class distribution in VAL set:\n",
      "aspect\n",
      "clarity_negative       625\n",
      "clarity_positive       196\n",
      "motivation_positive     22\n",
      "motivation_negative     14\n",
      "soundness_negative      11\n",
      "soundness_positive       3\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Class distribution in TEST set:\n",
      "aspect\n",
      "clarity_negative       1249\n",
      "clarity_positive        392\n",
      "motivation_positive      44\n",
      "motivation_negative      27\n",
      "soundness_negative       22\n",
      "soundness_positive        7\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Saved train.csv, val.csv, test.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ─── Load the prepared dataset ──────────────────────────────────────────\n",
    "df = pd.read_csv(\"/kaggle/working/phase2_dataset.csv\")\n",
    "\n",
    "# Display overall class distribution\n",
    "print(\"Overall aspect distribution:\")\n",
    "print(df[\"aspect\"].value_counts(), \"\\n\")\n",
    "\n",
    "# ─── 1) Split off final test set (20% of data) ─────────────────────────\n",
    "# We stratify by 'aspect' to keep class balance across splits.\n",
    "train_val, test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.20,\n",
    "    stratify=df[\"aspect\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train+Val shape: {train_val.shape}\")\n",
    "print(f\"Test shape:        {test.shape}\\n\")\n",
    "\n",
    "# ─── 2) Split train_val into train (80% of original) and val (10% of original) ─────────────────────────\n",
    "# Since train_val is 80% of original, using test_size=0.125 yields 10% of original for validation.\n",
    "train, val = train_test_split(\n",
    "    train_val,\n",
    "    test_size=0.125,\n",
    "    stratify=train_val[\"aspect\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Val shape:   {val.shape}\")\n",
    "print(f\"Test shape:  {test.shape}\\n\")\n",
    "\n",
    "# ─── 3) Verify class distribution in each split ─────────────────────────\n",
    "print(\"Class distribution in TRAIN set:\")\n",
    "print(train[\"aspect\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Class distribution in VAL set:\")\n",
    "print(val[\"aspect\"].value_counts(), \"\\n\")\n",
    "\n",
    "print(\"Class distribution in TEST set:\")\n",
    "print(test[\"aspect\"].value_counts(), \"\\n\")\n",
    "\n",
    "# ─── 4) Save splits to CSV for modeling ────────────────────────────────\n",
    "train.to_csv(\"train.csv\", index=False)\n",
    "val.to_csv(\"val.csv\",     index=False)\n",
    "test.to_csv(\"test.csv\",   index=False)\n",
    "\n",
    "print(\"Saved train.csv, val.csv, test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model - Zero shot with longllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:24:18.602350Z",
     "iopub.status.busy": "2025-05-27T14:24:18.602052Z",
     "iopub.status.idle": "2025-05-27T14:26:06.438029Z",
     "shell.execute_reply": "2025-05-27T14:26:06.437423Z",
     "shell.execute_reply.started": "2025-05-27T14:24:18.602325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 14:25:50.168902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748355950.359617      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748355950.411935      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & Installs\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "!pip install -q wandb evaluate transformers peft bitsandbytes rouge_score bert_score\n",
    "import os, random, pandas as pd, numpy as np, wandb\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bitsandbytes import __version__ as bnb_version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T14:26:06.439474Z",
     "iopub.status.busy": "2025-05-27T14:26:06.438866Z",
     "iopub.status.idle": "2025-05-27T14:26:12.819361Z",
     "shell.execute_reply": "2025-05-27T14:26:12.818650Z",
     "shell.execute_reply.started": "2025-05-27T14:26:06.439437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mesrasekerci\u001b[0m (\u001b[33mesrasekerci-metu-middle-east-technical-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"0e48c15605abf65402208cd05becaa061bf0dfbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:58:50.146788Z",
     "iopub.status.busy": "2025-05-27T16:58:50.146038Z",
     "iopub.status.idle": "2025-05-27T16:59:51.000436Z",
     "shell.execute_reply": "2025-05-27T16:59:50.999636Z",
     "shell.execute_reply.started": "2025-05-27T16:58:50.146763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 20482500 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2) Configuration & Seeds\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "PROJECT     = \"is584-phase2\"\n",
    "MODEL_ID    = \"syzymon/long_llama_3b_instruct\"\n",
    "TRAIN_FILE  = \"train.csv\"\n",
    "VAL_FILE    = \"val.csv\"\n",
    "TEST_FILE   = \"test.csv\"\n",
    "OUTPUT_DIR  = \"./phase2_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED        = 42\n",
    "BATCH       = 8\n",
    "MAX_IN      = 512\n",
    "MAX_OUT     = 64\n",
    "LORA_RANKS  = [8, 16]\n",
    "EPOCHS      = 1\n",
    "LR          = 2e-4\n",
    "DEVICE      = \"cuda\"\n",
    "set_seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# 3) Load Data Splits\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "df_train = pd.read_csv(TRAIN_FILE)\n",
    "df_val   = pd.read_csv(VAL_FILE)\n",
    "df_test  = pd.read_csv(TEST_FILE)\n",
    "\n",
    "# 4) Initialize Tokenizer & Base Model (FP16, GPU)\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:59:51.003820Z",
     "iopub.status.busy": "2025-05-27T16:59:51.003596Z",
     "iopub.status.idle": "2025-05-27T16:59:59.913484Z",
     "shell.execute_reply": "2025-05-27T16:59:59.912932Z",
     "shell.execute_reply.started": "2025-05-27T16:59:51.003803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.dropna(subset=['review'])\n",
    "df_val   = df_val.dropna(subset=['review'])\n",
    "df_test  = df_test.dropna(subset=['review'])\n",
    "\n",
    "# 5) Build Prompts & References\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "def build_prompts(df):\n",
    "    P, R = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        p = row.paper_text.replace(\"\\n\",\" \")[:2000]\n",
    "        rv= row.review.replace(\"\\n\",\" \")[:1000]\n",
    "        asp = row.aspect\n",
    "        prompt = (\n",
    "            f\"Paper Excerpt:\\n{p}\\n\\n\"\n",
    "            f\"Previous Review:\\n{rv}\\n\\n\"\n",
    "            f\"Aspect: {asp}\\n\\n\"\n",
    "            \"Task: Generate exactly one new sentence focusing on this aspect.\"\n",
    "        )\n",
    "        P.append(prompt)\n",
    "        R.append(row.aspect_sentence)\n",
    "    return P, R\n",
    "\n",
    "train_prompts, train_refs = build_prompts(df_train)\n",
    "val_prompts,   val_refs   = build_prompts(df_val)\n",
    "test_prompts,  test_refs  = build_prompts(df_test)\n",
    "\n",
    "# 6) Tokenize for Trainer\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "def tokenize_for_train(prompts, refs):\n",
    "    enc = tokenizer(prompts, truncation=True, max_length=MAX_IN,\n",
    "                    padding=\"max_length\", return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labs = tokenizer(refs, truncation=True, max_length=MAX_OUT,\n",
    "                         padding=\"max_length\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    enc[\"labels\"] = labs\n",
    "    return enc\n",
    "\n",
    "train_ds = tokenize_for_train(train_prompts, train_refs)\n",
    "val_ds   = tokenize_for_train(val_prompts,   val_refs)\n",
    "\n",
    "# 7) Data Collator & Metric Computation\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=base_model)\n",
    "\n",
    "bleu  = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    dec_p = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    dec_l = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    refs_wrapped = [[l] for l in dec_l]\n",
    "    b = bleu.compute(predictions=dec_p, references=refs_wrapped)[\"bleu\"]\n",
    "    r = rouge.compute(predictions=dec_p, references=dec_l,\n",
    "                      rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"])\n",
    "    return {\n",
    "        \"bleu\": b,\n",
    "        \"rouge1\": r[\"rouge1\"].mid.fmeasure,\n",
    "        \"rouge2\": r[\"rouge2\"].mid.fmeasure,\n",
    "        \"rougeL\": r[\"rougeL\"].mid.fmeasure\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T15:15:38.235670Z",
     "iopub.status.busy": "2025-05-27T15:15:38.235125Z",
     "iopub.status.idle": "2025-05-27T16:24:40.334903Z",
     "shell.execute_reply": "2025-05-27T16:24:40.334332Z",
     "shell.execute_reply.started": "2025-05-27T15:15:38.235646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zero-shot</strong> at: <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/uj2e748p' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/uj2e748p</a><br> View project at: <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250527_144200-uj2e748p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250527_151538-5fcy2yds</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/5fcy2yds' target=\"_blank\">zero-shot</a></strong> to <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/5fcy2yds' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/5fcy2yds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192d0a04cd634a528d043c97b15d1d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val zero-shot:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b273a29d3f485b9fb262cd6862dbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test zero-shot:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/zs_bleu</td><td>▁</td></tr><tr><td>test/zs_r1</td><td>▁</td></tr><tr><td>test/zs_r2</td><td>▁</td></tr><tr><td>test/zs_rL</td><td>▁</td></tr><tr><td>val/zs_bleu</td><td>▁</td></tr><tr><td>val/zs_r1</td><td>▁</td></tr><tr><td>val/zs_r2</td><td>▁</td></tr><tr><td>val/zs_rL</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/zs_bleu</td><td>0.00106</td></tr><tr><td>test/zs_r1</td><td>0.0965</td></tr><tr><td>test/zs_r2</td><td>0.00507</td></tr><tr><td>test/zs_rL</td><td>0.07692</td></tr><tr><td>val/zs_bleu</td><td>0.00139</td></tr><tr><td>val/zs_r1</td><td>0.09478</td></tr><tr><td>val/zs_r2</td><td>0.00514</td></tr><tr><td>val/zs_rL</td><td>0.07557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zero-shot</strong> at: <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/5fcy2yds' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2/runs/5fcy2yds</a><br> View project at: <a href='https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2' target=\"_blank\">https://wandb.ai/esrasekerci-metu-middle-east-technical-university/is584-phase2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250527_151538-5fcy2yds/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8) Zero-Shot Baseline (FP16 Generation + W&B + CSV)\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "wandb.init(project=PROJECT, name=\"zero-shot\", reinit=True)\n",
    "\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens=MAX_OUT,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def run_zero_shot(df, prompts, refs, split):\n",
    "    preds = []\n",
    "    # tqdm for inference progress\n",
    "    for i in tqdm(range(0, len(prompts), BATCH), desc=f\"{split} zero-shot\"):\n",
    "        batch = prompts[i : i + BATCH]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_IN\n",
    "        ).to(DEVICE)\n",
    "        outs = base_model.generate(**enc, generation_config=gen_cfg)\n",
    "        for seq, mask in zip(outs, enc.attention_mask):\n",
    "            L = mask.sum().item()\n",
    "            preds.append(tokenizer.decode(seq[L:], skip_special_tokens=True))\n",
    "\n",
    "    # Compute BLEU\n",
    "    b = bleu.compute(predictions=preds, references=[[r] for r in refs])[\"bleu\"]\n",
    "\n",
    "    # Compute ROUGE and normalize\n",
    "    raw = rouge.compute(predictions=preds, references=refs,\n",
    "                        rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"])\n",
    "    def _fm(val):\n",
    "        # if it’s a Score object, grab val.mid.fmeasure, else assume float\n",
    "        return getattr(val, \"mid\", val).fmeasure if hasattr(val, \"mid\") else float(val)\n",
    "    r1 = _fm(raw[\"rouge1\"])\n",
    "    r2 = _fm(raw[\"rouge2\"])\n",
    "    rL = _fm(raw[\"rougeL\"])\n",
    "\n",
    "    # Log to W&B\n",
    "    wandb.log({\n",
    "        f\"{split}/zs_bleu\": b,\n",
    "        f\"{split}/zs_r1\":   r1,\n",
    "        f\"{split}/zs_r2\":   r2,\n",
    "        f\"{split}/zs_rL\":   rL\n",
    "    })\n",
    "\n",
    "    # Save predictions\n",
    "    out = df.copy()\n",
    "    out[\"pred_zs\"] = preds\n",
    "    out.to_csv(f\"{OUTPUT_DIR}/{split}_zs.csv\", index=False)\n",
    "\n",
    "# Run with progress bars\n",
    "run_zero_shot(df_val,  val_prompts,  val_refs,  \"val\")\n",
    "run_zero_shot(df_test, test_prompts, test_refs, \"test\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7507648,
     "sourceId": 11942420,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
